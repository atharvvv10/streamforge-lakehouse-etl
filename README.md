ðŸš€ StreamForge Analytics Platform
Real-Time Data Lakehouse for High-Velocity Event Streams
ðŸŽ¯ I. Objective & Architectural Overview

StreamForge is a modern, production-ready Real-Time Data Lakehouse engineered for continuous ingestion, low-latency streaming transformations, scalable object storage, and distributed analytics.

Below is the fully bordered architecture table:

+---------------------+-------------------+----------------------------+---------------------------------------------------------------+
| Core Component      | Technology        | Service Name               | Purpose                                                       |
+---------------------+-------------------+----------------------------+---------------------------------------------------------------+
| Messaging Bus       | Apache Kafka      | streaming-server           | High-throughput, persistent event ingestion                  |
| Processing Layer    | Apache Flink      | stream-processor           | Stateful real-time ETL, filtering, enrichment                |
| Object Storage      | MinIO             | minio-storage-service      | S3-compatible durable storage                                |
| Table Format        | Apache Iceberg    | iceberg-catalog-svc        | ACID transactions, schema evolution, time travel             |
| SQL Query Engine    | Trino             | query-engine               | Distributed SQL execution on top of Iceberg tables           |
| Visualization Layer | Apache Superset   | viz-dashboard              | Dashboards, analytics exploration, BI capabilities           |
+---------------------+-------------------+----------------------------+---------------------------------------------------------------+

ðŸ“‚ II. Project File Structure
streamforge-analytics/
â”œâ”€â”€ data-emitter/                 
â”‚   â”œâ”€â”€ stream_source.py
â”‚   â”œâ”€â”€ python_deps.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ stream-processor/             
â”‚   â”œâ”€â”€ sql-client/
â”‚   â”‚   â”œâ”€â”€ flink_runtime.yaml
â”‚   â”‚   â””â”€â”€ cli_builder.Dockerfile
â”‚   â””â”€â”€ sql-jobs/
â”‚       â””â”€â”€ transform_pipeline.sql
â”‚
â”œâ”€â”€ query-engine/                 
â”‚   â””â”€â”€ iceberg_catalog.properties
â”‚
â”œâ”€â”€ viz-dashboard/                
â”‚   â”œâ”€â”€ web_config.py
â”‚   â”œâ”€â”€ init_superuser.sh
â”‚   â””â”€â”€ viz_app.docker
â”‚
â”œâ”€â”€ orchestrator.yml              
â””â”€â”€ COPYRIGHT.txt                 

ðŸ› ï¸ III. Environment Setup
Prerequisites
+------------------------+
|   REQUIRED SOFTWARE    |
+------------------------+
| Docker Desktop / Engine|
| Docker Compose v2+     |
| >= 16GB RAM recommended|
+------------------------+

1. Clone the Repository
git clone <your-repo-url>
cd streamforge-analytics

2. Launch Entire Platform
docker compose -f orchestrator.yml up --build -d

ðŸŒ IV. Access Endpoints
+-------------------------+--------------------------+-------------------------------+-------------------------------+
| Service                 | Container Name           | URL                           | Credentials                  |
+-------------------------+--------------------------+-------------------------------+-------------------------------+
| Flink Job Manager UI    | stream-job-master        | http://localhost:8084         | None                         |
| MinIO Console           | object-storage-svc       | http://localhost:9002         | minio-admin / minio-password-1|
| Trino Web UI           | trino-query-server       | http://localhost:8889         | None                         |
| Superset Dashboard      | data-visualization-app   | http://localhost:9099         | viz_master / superstrongpassword |
+-------------------------+--------------------------+-------------------------------+-------------------------------+

ðŸŒŠ V. Data Processing Flow
A. Event Generation â€” data-emitter/stream_source.py
+-------------------------------------------+
|   STREAM EVENT TYPES GENERATED            |
+-------------------------------------------+
| â€¢ Page Views                              |
| â€¢ Product Clicks                          |
| â€¢ Add-To-Cart Events                      |
| â€¢ Checkout Initiation                     |
| â€¢ Purchase Events                         |
+-------------------------------------------+


Synthetic JSON events â†’ published to Kafka topic:
user-activity-stream

B. Real-Time ETL â€” Flink SQL

Flink performs:

+---------------------------------------------------------------+
|                    FLINK ETL PIPELINE                        |
+---------------------------------------------------------------+
| SOURCE: Kafka topic 'user-activity-stream'                    |
| TRANSFORM: Filter purchases where transaction_value > 50      |
| SINK: Write into Iceberg table (Avro format via MinIO)        |
+---------------------------------------------------------------+

C. Querying Iceberg Tables (via Trino)

Access Trino shell:

docker compose -f orchestrator.yml exec query-engine trino --user analytics_user


Run SQL:

USE iceberg_data_lake.marketing_events;

SELECT *
FROM processed_conversions
LIMIT 5;

ðŸ“œ VI. Project Metadata
+----------------------+----------------------------------------+
| Field                | Details                                |
+----------------------+----------------------------------------+
| License              | MIT License (see COPYRIGHT.txt)        |
| Author               | Atharv Chougale Â© 2025                 |
| Project              | StreamForge Analytics Platform         |
+----------------------+----------------------------------------+

