# ğŸš€ StreamForge Lakehouse ETL
Modular, Scalable & Real-Time Lakehouse ETL Pipeline

streamforge-lakehouse-etl is a fully modular ETL framework designed for building modern Lakehouse + Streaming Data Pipelines.
Each component is decoupled into its own service so you can scale, replace, or extend parts independently â€” like a real production-grade data system.

## ğŸ§± Core Components Overview
| **Core Component**    | **Role**                        | **Technology**      | **Implementation (New Name)** | **Function**                                                     |
|------------------------|----------------------------------|----------------------|-------------------------------|------------------------------------------------------------------|
| **Messaging Bus**      | Event Ingestion & Decoupling     | Apache Kafka         | `streaming-server`            | Decoupled, fault-tolerant ingestion of real-time events.         |
| **Processing Layer**   | Stateful Stream Processing       | Apache Flink         | `stream-processor`            | Real-time ETL, filtering, transformation, and enrichment.        |
| **Object Storage**     | Persistent Data Lake Storage     | MinIO                | `minio-storage-service`       | S3-compatible object storage for raw + processed datasets.       |
| **Table Format**       | Lakehouse Table Management       | Apache Iceberg       | `iceberg-catalog-svc`         | Schema evolution + ACID transactions for lakehouse tables.       |
| **SQL Access**         | Distributed Query Engine         | Trino                | `query-engine`                | High-performance SQL querying over Iceberg tables.               |
| **Visualization**      | BI & Dashboarding               | Apache Superset      | `viz-dashboard`               | Interactive dashboards + visual analytics.                       |

## ğŸ“¦ Project Structure
streamforge-lakehouse-etl/
â”‚
â”œâ”€â”€ data-emitter/           # Simulated or real data ingestion layer
â”œâ”€â”€ stream-processor/       # Real-time streaming ETL logic
â”œâ”€â”€ query-engine/           # SQL query engine configuration (Trino)
â”œâ”€â”€ viz-dashboard/          # Dashboarding & BI (Superset)
â”œâ”€â”€ orchestrator.yml        # Full system orchestration
â””â”€â”€ LICENSE                 # MIT License

## ğŸ¯ Objective

To build a production-style data pipeline that supports:

Real-time event ingestion

Stateful stream processing

Lakehouse-style storage & governance

SQL analytics engine

Dashboarding for insights

All packaged into clear, modular components.

## âš™ï¸ Getting Started
```bash
1ï¸âƒ£ Clone the Repository
git clone https://github.com/atharvvv10/streamforge-lakehouse-etl.git
cd streamforge-lakehouse-etl

2ï¸âƒ£ Start Individual Modules

Each folder is self-contained.
Typical workflow:

ğŸ”¹ Start Kafka (streaming-server)

Produces and receives real-time clickstream/events.

ğŸ”¹ Start Flink (stream-processor)

Applies ETL transforms, filtering, enrichment.

ğŸ”¹ Start MinIO + Iceberg

Acts as your object store & table catalog.

ğŸ”¹ Start Trino (query-engine)

Allows you to query Iceberg tables using SQL.

ğŸ”¹ Start Superset (viz-dashboard)

Connects to Trino for dashboarding.

All services can be controlled through orchestrator.yml.
```

## ğŸ§© Why This Architecture?

ğŸ”„ Decoupled microservices â†’ scalable & replaceable

âš¡ Real-time ETL â†’ immediate transformations

ğŸ§Š Lakehouse support via Iceberg â†’ ACID + schema evolution

ğŸ” Interactive SQL queries â†’ Trino for fast analytics

ğŸ“Š Dashboards â†’ complete end-to-end visibility

This mirrors real-world modern data engineering setups.

## ğŸ›£ï¸ Roadmap / Future Features

Support for Delta Lake or Apache Hudi

Fully dockerized version

CI/CD integration

Automated schema registry

Orchestrator upgrade (Airflow / Dagster)

Machine learning feature-store layer

## ğŸ¤ Contributing

PRs are welcome!

Fork

Create feature branch

Commit changes

Open PR

## ğŸ“„ License

This project is licensed under the MIT License.
